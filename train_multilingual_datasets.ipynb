{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB9SXNf9Wru4",
        "outputId": "8be2c62f-3d38-4ddb-969d-821c92534e80"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVU_5NyinrVc",
        "outputId": "99adb76d-0574-4aa9-daf9-df57993ab6b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets evaluate wandb transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlWxYqbKW0Pq",
        "outputId": "e1ed99f9-9ecd-4adc-8839-b348ea604433"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/542.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/542.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m532.5/542.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "btOidS41WhHX"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, get_dataset_config_names\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import evaluate\n",
        "import wandb\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "NUM_PROC=5\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "id": "X4hPHgKnWrNK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRANSFORMER=\"google-bert/bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scPzAa4lXX-B",
        "outputId": "2f537444-a888-45d3-a572-32c711df94fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class StatementDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, statements, labels):\n",
        "        self.statements = statements\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encodings = tokenizer(self.statements[idx], truncation=True, padding=True)\n",
        "        item = {key: torch.tensor(val) for key, val in encodings.items()}\n",
        "        item['labels'] = int(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "LgFu6GUAXHbf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_base = 'mbzuai-ugrip-statement-tuning'\n",
        "dataset_names=['Topic-Statements', 'sentiments', 'wikilingual_dataset', 'massive', 'exams', 'xlwic', 'paws-x', 'belebele']"
      ],
      "metadata": {
        "id": "Ce3raR8-XqHI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# took 10 minutes to run - this will probably be replaced with a new dataset in the future, 2m if cached\n",
        "tolerance = 20\n",
        "# train : valid = 90:10 (800 -> 720:80): belebele is capped at 900\n",
        "TRAIN_SIZE=800\n",
        "TEST_SIZE=80\n",
        "\n",
        "# let's just append the StatementDatasets to this big list\n",
        "train_dataset = []\n",
        "val_dataset = []\n",
        "\n",
        "# this is to handle different naming conventions of different datasets\n",
        "langs = {'english': ['en', 'eng_Latn', 'english'], 'chinese': ['zh', 'zho_Hans', 'chinese'], 'vietnamese': ['vi', 'vie_Latn', 'vietnamese'], 'french': ['fr', 'fra_Latn', 'french']}\n",
        "\n",
        "for lang_key, lang_val in langs.items():\n",
        "  for dataset_name in dataset_names:\n",
        "\n",
        "    this_dataset_lang_codes = get_dataset_config_names(f\"{dataset_base}/{dataset_name}\")\n",
        "\n",
        "    if len(this_dataset_lang_codes[0]) == 2:\n",
        "      subset_lang = lang_val[0]\n",
        "    elif \"_\" in this_dataset_lang_codes[0]:\n",
        "      subset_lang = lang_val[1]\n",
        "    # 'arabic', 'chinese', 'english', 'french', 'german', 'hindi', 'indonesian', 'italian', 'japanese', 'malay', 'portuguese', 'spanish'\n",
        "    else:\n",
        "      subset_lang = lang_val[2]\n",
        "\n",
        "    # our language is unfortunately not available in this dataset, skip\n",
        "    if subset_lang not in this_dataset_lang_codes:\n",
        "      print(f\"Skipping {lang_key} in {dataset_name}... because {this_dataset_lang_codes}\")\n",
        "      continue\n",
        "\n",
        "    data = load_dataset(f\"{dataset_base}/{dataset_name}\", subset_lang)\n",
        "\n",
        "    # eliminate column naming disparity\n",
        "    if 'label' in data['train'].features and 'is_true' not in data['train'].features:\n",
        "      data = data.rename_column('label', 'is_true')\n",
        "\n",
        "    assert('label' not in data['train'].features)\n",
        "\n",
        "    # example['is_true'] is not None AND len(tokenizer(example['statement']['input_ids']) < 512)\n",
        "    train = data['train'].filter(lambda example: example[\"is_true\"] is not None).filter(lambda example: len(tokenizer(example['statement'])['input_ids']) < 512)\n",
        "\n",
        "    train = train.train_test_split(test_size=TRAIN_SIZE)['test']\n",
        "    train_statements, val_statements, train_labels, val_labels = train_test_split(train['statement'], train['is_true'], test_size=TEST_SIZE, random_state=SEED, shuffle=True)\n",
        "\n",
        "    this_train_dataset = StatementDataset(train_statements, train_labels)\n",
        "    this_val_dataset = StatementDataset(val_statements, val_labels)\n",
        "\n",
        "    train_dataset.append(this_train_dataset)\n",
        "    val_dataset.append(this_val_dataset)\n",
        "\n",
        "    print(f\"Dataset: {dataset_name} Language: {lang_key}\")\n",
        "    print(f\"Train size: {len(this_train_dataset)}\")\n",
        "    print(f\"Test size: {len(this_val_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUvAwLjZYHxh",
        "outputId": "ad4f039b-6671-42aa-b405-33a360de1f88"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: Topic-Statements Language: english\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: sentiments Language: english\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: wikilingual_dataset Language: english\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: massive Language: english\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Skipping english in exams... because ['bg', 'hr', 'hu', 'it', 'mk', 'pl', 'pt', 'sq', 'sr', 'tr', 'vi']\n",
            "Dataset: xlwic Language: english\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: paws-x Language: english\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: belebele Language: english\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: Topic-Statements Language: chinese\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: sentiments Language: chinese\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: wikilingual_dataset Language: chinese\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: massive Language: chinese\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Skipping chinese in exams... because ['bg', 'hr', 'hu', 'it', 'mk', 'pl', 'pt', 'sq', 'sr', 'tr', 'vi']\n",
            "Skipping chinese in xlwic... because ['de', 'en', 'fr', 'it']\n",
            "Dataset: paws-x Language: chinese\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: belebele Language: chinese\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: Topic-Statements Language: vietnamese\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Skipping vietnamese in sentiments... because ['arabic', 'chinese', 'english', 'french', 'german', 'hindi', 'indonesian', 'italian', 'japanese', 'malay', 'portuguese', 'spanish']\n",
            "Dataset: wikilingual_dataset Language: vietnamese\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: massive Language: vietnamese\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: exams Language: vietnamese\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Skipping vietnamese in xlwic... because ['de', 'en', 'fr', 'it']\n",
            "Skipping vietnamese in paws-x... because ['de', 'en', 'es', 'fr', 'ja', 'ko', 'zh']\n",
            "Dataset: belebele Language: vietnamese\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: Topic-Statements Language: french\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: sentiments Language: french\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: wikilingual_dataset Language: french\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: massive Language: french\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Skipping french in exams... because ['bg', 'hr', 'hu', 'it', 'mk', 'pl', 'pt', 'sq', 'sr', 'tr', 'vi']\n",
            "Dataset: xlwic Language: french\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: paws-x Language: french\n",
            "Train size: 720\n",
            "Test size: 80\n",
            "Dataset: belebele Language: french\n",
            "Train size: 720\n",
            "Test size: 80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is a list of StatementDataset objects\n",
        "print(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd0R6oshckw3",
        "outputId": "a791f650-4b6a-4d2a-dd93-2862d3987a86"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<__main__.StatementDataset object at 0x7cd569e459f0>, <__main__.StatementDataset object at 0x7cd57ab5c130>, <__main__.StatementDataset object at 0x7cd580d31fc0>, <__main__.StatementDataset object at 0x7cd57bfcd630>, <__main__.StatementDataset object at 0x7cd5797cd0c0>, <__main__.StatementDataset object at 0x7cd580e50d00>, <__main__.StatementDataset object at 0x7cd57aaaef80>, <__main__.StatementDataset object at 0x7cd570602260>, <__main__.StatementDataset object at 0x7cd5702b9c60>, <__main__.StatementDataset object at 0x7cd58128e1d0>, <__main__.StatementDataset object at 0x7cd58119eb30>, <__main__.StatementDataset object at 0x7cd569e50e50>, <__main__.StatementDataset object at 0x7cd57025c0d0>, <__main__.StatementDataset object at 0x7cd569e0d8d0>, <__main__.StatementDataset object at 0x7cd5810adf60>, <__main__.StatementDataset object at 0x7cd56982c6d0>, <__main__.StatementDataset object at 0x7cd6c9a52740>, <__main__.StatementDataset object at 0x7cd580de5d50>, <__main__.StatementDataset object at 0x7cd57976c5b0>, <__main__.StatementDataset object at 0x7cd58206a920>, <__main__.StatementDataset object at 0x7cd5811c36d0>, <__main__.StatementDataset object at 0x7cd5703c6260>, <__main__.StatementDataset object at 0x7cd57ab45c90>, <__main__.StatementDataset object at 0x7cd569ac04c0>, <__main__.StatementDataset object at 0x7cd580e7f400>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten\n",
        "flat_train_statements = [statement for dataset in train_dataset for statement in dataset.statements]\n",
        "flat_train_labels = [label for dataset in train_dataset for label in dataset.labels]\n",
        "\n",
        "flat_val_statements = [statement for dataset in val_dataset for statement in dataset.statements]\n",
        "flat_val_labels = [label for dataset in val_dataset for label in dataset.labels]\n",
        "\n",
        "# Create a new dataset with the flattened lists\n",
        "flat_train_dataset = StatementDataset(flat_train_statements, flat_train_labels)\n",
        "flat_val_dataset = StatementDataset(flat_val_statements, flat_val_labels)"
      ],
      "metadata": {
        "id": "iVaqa3zKnAVQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
      ],
      "metadata": {
        "id": "uAU_prounx6x"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    # Decode generated summaries, which is in ids into text\n",
        "    _, predictions = torch.max(torch.tensor(predictions), dim=1)\n",
        "\n",
        "\n",
        "    return clf_metrics.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "DQwrOz1ZoARh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)"
      ],
      "metadata": {
        "id": "rLcLQZc4oBmf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_TRAIN_EPOCHS=6\n",
        "TRAINING_BATCH=32\n",
        "WARMUP=0.1\n",
        "LEARNING_RATE=2e-06\n",
        "EXPERIMENT_NAME=\"MBERT\"\n",
        "DECAY=0.01"
      ],
      "metadata": {
        "id": "qRSszJ9doGRS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP-ZcnLZoyvb",
        "outputId": "70c46b5d-236e-49c0-bd56-97c38872c748"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(\n",
        "    entity=\"mbzuai-ugrip\",\n",
        "    project=f\"{EXPERIMENT_NAME}_multilingual_train\",\n",
        "    name=f\"{EXPERIMENT_NAME}_{LEARNING_RATE}_{TRAINING_BATCH}_{WARMUP}_{DECAY}\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "OKwAXNmHwKsM",
        "outputId": "aec88137-de1e-4961-e9c6-9bf1ae8d0ca0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msoph20\u001b[0m (\u001b[33mmbzuai-ugrip\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240610_104615-4xy8k0fl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mbzuai-ugrip/MBERT_multilingual_train/runs/4xy8k0fl' target=\"_blank\">MBERT_2e-06_32_0.1_0.01</a></strong> to <a href='https://wandb.ai/mbzuai-ugrip/MBERT_multilingual_train' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mbzuai-ugrip/MBERT_multilingual_train' target=\"_blank\">https://wandb.ai/mbzuai-ugrip/MBERT_multilingual_train</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mbzuai-ugrip/MBERT_multilingual_train/runs/4xy8k0fl' target=\"_blank\">https://wandb.ai/mbzuai-ugrip/MBERT_multilingual_train/runs/4xy8k0fl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mbzuai-ugrip/MBERT_multilingual_train/runs/4xy8k0fl?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7a21455d3c10>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=f'./{EXPERIMENT_NAME}-outputs',          # output directory\n",
        "    num_train_epochs=NUM_TRAIN_EPOCHS,              # total number of training epochs (num train epochs: 6)\n",
        "    per_device_train_batch_size=TRAINING_BATCH,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    warmup_ratio=WARMUP,                # number of warmup steps for learning rate scheduler\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=DECAY,               # strength of weight decay\n",
        "    logging_dir=f'./{EXPERIMENT_NAME}-logs',            # directory for storing logs\n",
        "    logging_steps=1000,\n",
        "    save_steps=1000,\n",
        "    evaluation_strategy='steps',\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end= True,\n",
        "    metric_for_best_model='f1',\n",
        "    report_to=\"wandb\",\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(TRANSFORMER)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=flat_train_dataset,         # training dataset\n",
        "    compute_metrics=compute_metrics,\n",
        "    eval_dataset=flat_val_dataset,            # evaluation dataset\n",
        "    data_collator=data_collator\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eompaVboDMh",
        "outputId": "4395cac9-cf7d-42b9-aa29-6a32a626f972"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        },
        "id": "9dJTRb1KoEsX",
        "outputId": "d0350baf-1c4b-4c52-d977-5228adccb27a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='238' max='3378' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 238/3378 09:28 < 2:06:00, 0.42 it/s, Epoch 0.42/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(f\"sophiayk/m2-statement-tuning-mbert-multilingual-lr{LEARNING_RATE}-bs{TRAINING_BATCH}\")"
      ],
      "metadata": {
        "id": "OBkIAlZTo-J3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}