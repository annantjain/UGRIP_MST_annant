{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "lang_accuracies = {}\n",
    "for lang in langs:\n",
    "    print(f\"Processing {lang}...\")\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    for batch in tqdm(xnli_dataloaders[lang]):\n",
    "\n",
    "        tok1 = tokenizer(batch['statement1'], return_tensors='pt', padding=True).to(device)\n",
    "        tok2 = tokenizer(batch['statement2'], return_tensors='pt', padding=True).to(device)\n",
    "        tok3 = tokenizer(batch['statement3'], return_tensors='pt', padding=True).to(device)\n",
    "        labels = batch['label']\n",
    "\n",
    "        prob1 = F.softmax(model(input_ids=tok1['input_ids'], attention_mask=tok1['attention_mask']).logits, dim=-1)[:,1]\n",
    "        prob2 = F.softmax(model(input_ids=tok2['input_ids'], attention_mask=tok2['attention_mask']).logits, dim=-1)[:,1]\n",
    "        prob3 = F.softmax(model(input_ids=tok3['input_ids'], attention_mask=tok3['attention_mask']).logits, dim=-1)[:,1]\n",
    "\n",
    "         # Stack probabilities into a tensor along the last dimension\n",
    "        stacked_probs = torch.stack([prob1, prob2, prob3], dim=-1)\n",
    "\n",
    "        # Find the index of the maximum probability for each example in the batch\n",
    "        preds = torch.argmax(stacked_probs, dim=-1)\n",
    "        \n",
    "        predictions.extend(preds.cpu().tolist())\n",
    "        actual_labels.extend(labels.cpu().tolist())\n",
    "    lang_accuracies[lang] = clf_metrics.compute(predictions=predictions, references=actual_labels)['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XWinoGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xwinograd_dataloaders = {}\n",
    "for lang in langs:\n",
    "    xwinograd_dataloaders[lang] = DataLoader(xwinograd_statements[lang], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "lang_accuracies = {}\n",
    "\n",
    "for lang in langs:\n",
    "    print(f\"Processing {lang}...\")\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    for batch in tqdm(xwinograd_dataloaders[lang]):\n",
    "\n",
    "        tok1 = tokenizer(batch['statement1'], return_tensors='pt', padding=True).to(device)\n",
    "        tok2 = tokenizer(batch['statement2'], return_tensors='pt', padding=True).to(device)\n",
    "\n",
    "        labels = batch['answer']\n",
    "        prob1 = F.softmax(model(input_ids=tok1['input_ids'], attention_mask=tok1['attention_mask']).logits, dim=-1)[:,1]\n",
    "        prob2 = F.softmax(model(input_ids=tok2['input_ids'], attention_mask=tok2['attention_mask']).logits, dim=-1)[:,1]\n",
    "        \n",
    "        preds = torch.argmax(torch.stack([prob1, prob2],dim=-1),dim=-1)\n",
    "        predictions.extend(preds.cpu().tolist())\n",
    "        actual_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    lang_accuracies[lang] = clf_metrics.compute(predictions=predictions, references=actual_labels)['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XstoryCloze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xstorycloze_dataloaders = {}\n",
    "for lang in langs:\n",
    "    xstorycloze_dataloaders[lang] = DataLoader(xstorycloze_statements[lang], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "lang_accuracies = {}\n",
    "\n",
    "for lang in langs:\n",
    "    print(f\"Processing {lang}...\")\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    for batch in tqdm(xwinograd_dataloaders[lang]):\n",
    "\n",
    "        tok1 = tokenizer(batch['statement1'], return_tensors='pt', padding=True).to(device)\n",
    "        tok2 = tokenizer(batch['statement2'], return_tensors='pt', padding=True).to(device)\n",
    "\n",
    "        labels = batch['answer_right_ending']\n",
    "        prob1 = F.softmax(model(input_ids=tok1['input_ids'], attention_mask=tok1['attention_mask']).logits, dim=-1)[:,1]\n",
    "        prob2 = F.softmax(model(input_ids=tok2['input_ids'], attention_mask=tok2['attention_mask']).logits, dim=-1)[:,1]\n",
    "        \n",
    "        preds = torch.argmax(torch.stack([prob1, prob2],dim=-1),dim=-1)\n",
    "        predictions.extend(preds.cpu().tolist())\n",
    "        actual_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    lang_accuracies[lang] = clf_metrics.compute(predictions=predictions, references=actual_labels)['accuracy']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
