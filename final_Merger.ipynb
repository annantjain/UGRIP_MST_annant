{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m0JJQvj6Dwa"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset,get_dataset_config_names,load_from_disk, Dataset, DatasetDict, concatenate_datasets\n",
        "import random\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from sklearn.utils import resample\n",
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def datamaker(setlist, sz, lang,langlong,langfull):\n",
        "  train_lang = []\n",
        "  ddlang = []\n",
        "  listlen = 0\n",
        "  for s in setlist:\n",
        "    print(\"Processing: \" + s)\n",
        "    if (lang in get_dataset_config_names('mbzuai-ugrip-statement-tuning/'+s)) or (langlong in get_dataset_config_names('mbzuai-ugrip-statement-tuning/'+s)) or (langfull in get_dataset_config_names('mbzuai-ugrip-statement-tuning/'+s)):\n",
        "      if s == \"Topic-Statements\" or s == \"belebele\":\n",
        "        dataset = load_dataset('mbzuai-ugrip-statement-tuning/'+s, langfull, split='train', streaming=True)\n",
        "      elif s == \"sentiments\":\n",
        "        dataset = load_dataset('mbzuai-ugrip-statement-tuning/'+s, langlong, split='train', streaming=True)\n",
        "      else:\n",
        "        dataset = load_dataset('mbzuai-ugrip-statement-tuning/'+s, lang, split='train', streaming=True)\n",
        "      train_lang.append(dataset.take(sz))\n",
        "      #listlen += 1\n",
        "      print(\"Finished: \"+s)\n",
        "    else:\n",
        "      print(lang+\" not found in \" + s)\n",
        "      pass\n",
        "\n",
        "  print(\"Starting Merging\")\n",
        "\n",
        "  for i in train_lang:\n",
        "    ds = Dataset.from_generator(lambda: (yield from i), features=i.features)\n",
        "    dd = DatasetDict({\"train\": ds})\n",
        "    ddlang.append(dd)\n",
        "  trainset = ddlang[0][\"train\"]\n",
        "  for i in range(1,len(ddlang)-1):\n",
        "    trainset = concatenate_datasets([trainset,ddlang[i][\"train\"]])\n",
        "  return DatasetDict({\"train\": trainset})"
      ],
      "metadata": {
        "id": "GX5NBqtp6EU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fullmaker(length, langlist):\n",
        "  langdata = []\n",
        "  for lang in langlist:\n",
        "    langshort,langfull,langlong = lang\n",
        "    print(\"-----------------Gathering: \" + langlong)\n",
        "    data = datamaker(sets,length,langshort,langfull,langlong)\n",
        "    langdata.append(data)\n",
        "\n",
        "  print(\"--------------------------------Making Final Dataset----------------------------------------\")\n",
        "  finalset = langdata[0][\"train\"]\n",
        "  for i in range(1,len(langdata)-1):\n",
        "    finalset = concatenate_datasets([finalset,langdata[i][\"train\"]])\n",
        "\n",
        "  finaldataset = []\n",
        "  for i in range(len(finalset)):\n",
        "    thing = {}\n",
        "    if finalset[i][\"statement\"] != None:\n",
        "      thing[\"Text\"] = finalset[i][\"statement\"]\n",
        "    elif finalset[i][\"text\"] != None:\n",
        "      thing[\"Text\"] = finalset[i][\"text\"]\n",
        "    else:\n",
        "      print(\"missing text?\",i)\n",
        "\n",
        "    if finalset[i][\"is_true\"] != None:\n",
        "      thing[\"label\"] = finalset[i][\"is_true\"]\n",
        "    elif finalset[i][\"label\"] != None:\n",
        "      thing[\"label\"] = finalset[i][\"label\"]\n",
        "    else:\n",
        "      print(\"missing label?\",i)\n",
        "\n",
        "    finaldataset.append(thing)\n",
        "  return finaldataset"
      ],
      "metadata": {
        "id": "lQmGpjo86HKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sets = [\"xquad\", \"xlwic\", \"massive\", \"wikilingual_dataset\",\"paws-x\",\"Topic-Statements\",\"belebele\",\"sentiments\",\"exams\"]\n",
        "langlist = [(\"zh\",\"chinese\",\"zho_Hans\"), (\"en\",\"english\",\"eng_Latn\"),(\"fr\",\"french\",\"fra_Latn\"),(\"vi\",\"vietnamese\",\"vie_Latn\")]\n",
        "\n",
        "finalset = fullmaker(850,langlist)"
      ],
      "metadata": {
        "id": "4Y8v0uEf6JPB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}